Logging:
	- Logging server
	- Logging protocol definition (simple)
	- Logging to file
Request receiving over PULL sockets
Request receiving over SUB sockets
Advanced options for table opening, especially n-bits-per-key bloom filters
Dynamic spawning of update/read threads
Post-office-style read-and-update queues, but check performance beforehand
Copying from one table to another (important for PULL-based load balancing)
Multitable defragmentation
Centralized KV controller with keyspace-division (also related to defragmentation)
Map-Reduce Kernel:
	- LLVM JIT interface: Receive LLVM BC over ZMQ and execute that in spawned threads
	- Protocol definitions
	- Architectural diagram
	- Functionality to scan over a KV table (snapshot!) and pipe that into:
		* Any ZMQ PULL socket (especially multiple ones)
		* Directly to LLVM (probably over inproc:// PUSH/PULL)
Graph Kernel:
	- Re-use code from LevelOSM
	- Model: Directed graph with arbitrary node IDs
	- Probably it is better not to use binary node IDs, because escaping tends to cause reallocations
	- 
	- Hypergraph shall not be modeled directly in the database
	- Table format, multiple KV tables shall save:
		- 1 table adjacency list <node id><direction identifier><other node> --> edge attributes
		- 1 table node --> node attributes, nodes also need to be saved if they don't have attributes!
	- Define how attributes are saved:
		* Protobuf: Extra parsing step, probably too memory-intensive (multiple data copies) blah blah
		* ZMQ encoded message: --> zmsg_encode --> it needs some extra memory, but message decoding should be really fast,
			so this seems like the best option
		* For some applications attributes won't fit into memory or that would be too slow.
			--> provide "external attributes" that are saved somewhere else		
	- External protocol definition (should be similar to the KV protocol)
	- Graph mapreduce: Probably best if this is implemented as a wrapper for the normal mapreduce
	- Centralized graph controller that does keyspace division
		* Everything belonging to one node needs to be saved in one key-value db instance
		* (Difficult) Develop metric for efficient partitioning
